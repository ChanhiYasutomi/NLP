{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリーのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 形態素解析と共起ネットワークを描くためのインストール\n",
    "# !pip install SudachiPy\n",
    "# !pip install japanize-matplotlib\n",
    "# !pip install pyvis\n",
    "# !pip install spacy\n",
    "\n",
    "# # 言語モデル(詳細については以下のurlを確認)\n",
    "# pip install ja_core_news_sm\n",
    "# pip install ja_core_news_md\n",
    "# pip install ja_core_news_lg\n",
    "# pip install ja_core_news_trf\n",
    "# pip install ja-ginza\n",
    "# pip install ja_ginza_electra\n",
    "# pip install spacy_stanza\n",
    "\n",
    "# 言語モデルの比較: https://zenn.dev/akimen/articles/8d818ca704f079#%E6%AF%94%E8%BC%83%E7%B5%90%E6%9E%9C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "#共起ネットワーク\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import japanize_matplotlib\n",
    "from matplotlib.pyplot import figure, text\n",
    "\n",
    "# 言語モデル(辞書)を定義\n",
    "nlp = spacy.load('ja_ginza_electra') #GiNZA 日本語NLPライブラリ（transformerバージョン)らしい\n",
    "# transformerとは: https://www.youtube.com/watch?v=QlfhLMUoJjw\n",
    "\n",
    "# pandasの表示行数と列数を設定\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブル読み込み\n",
    "questionnaire_excel_file_path = r'xxx.xlsx'\n",
    "\n",
    "df = pd.read_excel(questionnaire_excel_file_path, sheet_name='xxx')\n",
    "df_whole = pd.read_excel(questionnaire_excel_file_path, sheet_name='yyy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重複削除(※任意)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ重複削除\n",
    "display(len(df))\n",
    "df = df.drop_duplicates('player_id')\n",
    "display(len(df))\n",
    "\n",
    "# 重複無し確認\n",
    "df[df['player_id'].duplicated(keep=False)].sort_values('player_id')\n",
    "df.to_excel(r\"xxx.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理の関数\n",
    "def preprocess_texts(text):\n",
    "    replaced_text = unicodedata.normalize(\"NFKC\", text)\n",
    "    replaced_text = replaced_text.upper()\n",
    "    replaced_text = re.sub(r'[【】 () （） 『』　「」]', '', replaced_text) #【】 () 「」　『』の除去\n",
    "    replaced_text = re.sub(r'[\\[\\]［］]', '', replaced_text)\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) # 数字を0にする\n",
    "    return replaced_text\n",
    "\n",
    "def nlp_preprocess(df):\n",
    "    df.iloc[:, 2] = (\n",
    "        df.iloc[:, 2]\n",
    "        .str.replace('xxx', 'yyy')\n",
    "        .apply(preprocess_texts)  # preprocess_texts 関数の呼び出し\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ↓共起ネットワークに関する関数_開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言語モデル\n",
    "# nlp = spacy.load(\"ja_ginza_electra\") #Transformerだからか時間がかかる\n",
    "nlp = spacy.load(\"ja_ginza\") #軽量に判定するためこちらを採用\n",
    "\n",
    "# 各行のテキスト内で重複する単語を除去する関数\n",
    "def remove_duplicate_words(text):\n",
    "    doc = nlp(text)\n",
    "    unique_words = set()\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.text not in unique_words:\n",
    "            unique_words.add(token.text)\n",
    "            filtered_tokens.append(token.text)\n",
    "    # 単語間にスペースを挿入して連結\n",
    "    return ''.join(filtered_tokens)\n",
    "\n",
    "# 上記の関数を適用\n",
    "def process_text_column(df):\n",
    "    df.iloc[:,2] = df.iloc[:,2].apply(remove_duplicate_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書に単語を追加\n",
    "# ファイルを読み込む\n",
    "dictionary_path = r'xxx.txt'\n",
    "\n",
    "with open(dictionary_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# 文字列をスペースで分割して単語リストにする\n",
    "words = content.split()\n",
    "\n",
    "# patterns リストを作成\n",
    "patterns = [{\"label\": \"NOUN\", \"pattern\": word} for word in words]\n",
    "\n",
    "config = {\n",
    "   'overwrite_ents': True\n",
    "}\n",
    "\n",
    "# 辞書への追加(patterns リスト追加)\n",
    "ruler = nlp.add_pipe('entity_ruler', config=config)\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書がされているのか確認\n",
    "doc = nlp('xxx')\n",
    "doc = nlp(content)\n",
    "\n",
    "# 単語依存構造の可視化\n",
    "displacy.render(doc, style=\"dep\", options={\"compact\":True},  jupyter=True)\n",
    "\n",
    "# エンティティ抽出\n",
    "displacy.render(doc, style=\"ent\", options={\"compact\":True},  jupyter=True)\n",
    "\n",
    "# 以下から辞書に登録されていることが確認できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードをダウンロード&保存する関数\n",
    "def download_stopwords(url, save_path):\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords = [word.replace('\\n', '') for word in file.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# ストップワードのURLと保存パス\n",
    "stopword_url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "save_path = 'Japanese_Stopwords.txt'\n",
    "\n",
    "# ストップワードのダウンロードと保存\n",
    "download_stopwords(stopword_url, save_path)\n",
    "\n",
    "# ストップワードの読み込み\n",
    "japanese_stopwords = load_stopwords(save_path)\n",
    "\n",
    "# データフレーム\n",
    "df_japanese_stopwords = pd.DataFrame(japanese_stopwords, columns=['words'])\n",
    "\n",
    "# ストップワードを追加する場合\n",
    "stopwards_list = [['こと']]\n",
    "japanese_stopwords.append(stopwards_list)\n",
    "\n",
    "# 例: 読み込んだストップワードの表示\n",
    "display(len(japanese_stopwords))\n",
    "display(japanese_stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞を選択する関数\n",
    "def extract_words(sent, pos_tags):\n",
    "    words = [token.lemma_ for token in sent\n",
    "             if token.pos_ in pos_tags and token.lemma_ ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列を作成する関数\n",
    "def count_cooccurrence(sents, token_length='{2,}'):\n",
    "    token_pattern=f'\\\\b\\\\w{token_length}\\\\b'\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1\n",
    "    Xc = (X.T * X)\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セグメント分けして文章作成する関数\n",
    "def process_segment_texts(df, segment):\n",
    "    df_segment_text = df[df['segment'] == segment]\n",
    "    df_segment_text = '。'.join(df_segment_text.iloc[:, 2].astype(str))\n",
    "    return df_segment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とその頻度を算出する関数\n",
    "def tokenize_and_calculate_cooccurrence(doc):\n",
    "    # トークナイズの実行\n",
    "    docs = []\n",
    "    docs.append(nlp(doc))\n",
    "\n",
    "    # 共起を算出\n",
    "    # 品詞を指定します\n",
    "    include_pos = ('NOUN', 'ADJ', 'PROPN')\n",
    "    sents = []\n",
    "    for doc in docs:\n",
    "        sents.extend([' '.join(extract_words(sent, include_pos))\n",
    "                    for sent in doc.sents])\n",
    "    \n",
    "    words, word_counts, Xc, X = count_cooccurrence(sents, token_length='{2,}')\n",
    "\n",
    "    # NumPyの配列をDataFrameに変換\n",
    "    df_words = pd.DataFrame(words, columns=['words'])\n",
    "    df_word_counts = pd.DataFrame(word_counts, columns=['word_counts'])\n",
    "\n",
    "    # df_slr_words と df_slr_word_counts を結合\n",
    "    df_word_counts = pd.concat([df_words, df_word_counts], axis=1)\n",
    "    df_word_counts = df_word_counts.sort_values('word_counts', ascending=False)\n",
    "\n",
    "    df_slr_growth_rate = df_word_counts[~df_word_counts['words'].isin(df_japanese_stopwords['words'])].sort_values('word_counts', ascending=False)\n",
    "    # df_slr_growth_rate = df_slr_growth_rate[df_slr_growth_rate['word_counts'] >= 2]\n",
    "\n",
    "    return df_slr_growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の共起情報を抽出する関数\n",
    "def tokenize_and_extract_words(doc):\n",
    "    # トークナイズの実行\n",
    "    docs = []\n",
    "    docs.append(nlp(doc))\n",
    "\n",
    "    # 共起を算出\n",
    "    # 品詞を指定します\n",
    "    include_pos = ('NOUN', 'ADJ', 'PROPN')\n",
    "    sents = []\n",
    "    for doc in docs:\n",
    "        sents.extend([' '.join(extract_words(sent, include_pos))\n",
    "                    for sent in doc.sents])\n",
    "    \n",
    "    words, word_counts, Xc, X = count_cooccurrence(sents, token_length='{2,}')\n",
    "    \n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各単語に対して重みを計算する関数\n",
    "def word_weights(words, word_counts):\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {'weight': count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列から得られる単語の共起関係の重みを計算するための関数\n",
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i,j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i,j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起ネットワークを作成する関数\n",
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "\n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetworkX ライブラリを使用して与えられたグラフ G を可視化するための関数\n",
    "def pyplot_network(G):\n",
    "    plt.figure(figsize=(12, 8),dpi=300)\n",
    "    if layout == 'spring':\n",
    "      pos = nx.spring_layout(G, k=layout_parameter_k,iterations=50, threshold=0.0001, weight='weight', scale=1, center=None, dim=2, seed=123)\n",
    "\n",
    "    elif layout == 'fruchterman_reingold':\n",
    "      pos = nx.fruchterman_reingold_layout(G, dim=2, k=layout_parameter_k, pos=None, fixed=None, iterations=50, weight='weight', scale=1.0, center=None)\n",
    "\n",
    "    elif layout == 'kamada_kawai':\n",
    "      pos = nx.kamada_kawai_layout(G, dist=None, pos=None, weight=1, scale=1, center=None, dim=2)\n",
    "\n",
    "    else:\n",
    "      pos = nx.random_layout(G, center=None, dim=2, seed=None)\n",
    "\n",
    "    connecteds = []\n",
    "    colors_list = []\n",
    "\n",
    "    for i, c in enumerate(greedy_modularity_communities(G)):\n",
    "        connecteds.append(c)\n",
    "        colors_list.append(1/20 * i)\n",
    "\n",
    "    # ノードの色をカラーマップを使用して変更\n",
    "    colors_array = cm.Pastel1(np.linspace(0.1, 0.9, len(colors_list)))\n",
    "\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        for i, c in enumerate(connecteds):\n",
    "            if node in c:\n",
    "                node_colors.append(colors_array[i])\n",
    "                break\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_nodes(G, pos, alpha=0.5,node_color=node_colors,node_size=7000 * weights_n) # alpha = 0.5\n",
    "\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "    # nx.draw_networkx_edges(G, pos, edge_color=\"whitesmoke\", width=20 * weights_e) # alpha = 0.5\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=\"lightblue\", width=5 * weights_e)  # alpha = 0.5\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words, df_word_counts, df_Xc, df_X = tokenize_and_extract_words(df_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UIを使って、共起ネットワークの表示を変えられない理由\n",
    "# └ Visual Studio Codeでは別で対話的にパラメータを選択する設定をしないといけないらしいから\n",
    "#   └ Google Colab では、セル内のマジックコマンドを使って対話的にパラメータを選択できるが、VS Codeでは通常、スクリプト全体を実行するか、デバッグ機能を使用することが一般的\n",
    "\n",
    "# @title 共起ネットワーク表示\n",
    "layout = 'spring' #@param ['spring','fruchterman_reingold','kamada_kawai']\n",
    "weight_cutoff = 0.09 #@param {type:\"slider\", min:0.005, max:0.20, step:0.005}\n",
    "node_size_ = 500 #@param {type:\"slider\", min:1000, max:8000, step:500}\n",
    "text_size = 13 #@param {type:\"slider\", min:5, max:30, step:1}\n",
    "layout_parameter_k  = 0.3 #@param {type:\"slider\", min:0.01, max:1, step:0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの生成\n",
    "G_df_slr = create_network(df_words, df_word_counts, df_Xc, weight_cutoff)\n",
    "\n",
    "# 静的ネットワークの描画\n",
    "pyplot_network(G_df_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ↑共起ネットワークに関する関数_終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セグメント別UUを求める関数\n",
    "def uu_per_segment(df):\n",
    "    \n",
    "    # 満足度別回答数\n",
    "    df_large = df.groupby(['xxx', 'segment'], dropna=False, as_index=False).count().sort_values(['segment'], ascending=False)\n",
    "    \n",
    "    # 回答数\n",
    "    df_answer_cnt = pd.DataFrame([len(df)], columns=['回答数'])\n",
    "\n",
    "    # 回答率\n",
    "    df_answer_rate = pd.DataFrame([len(df) / len(df_whole)], columns=['回答率'])\n",
    "\n",
    "    # セグメント別回答数\n",
    "    df_segmnet_1_count = pd.DataFrame([df[df['segment'] == 'xxx'].iloc[:, 2].count()], columns=['xxx'])\n",
    "    df_segmnet_2_count = pd.DataFrame([df[df['segment'] == 'yyy'].iloc[:, 2].count()], columns=['yyy'])\n",
    "    df_segmnet_3_count = pd.DataFrame([df[df['segment'] == 'zzz'].iloc[:, 2].count()], columns=['zzz'])\n",
    "\n",
    "    # 上記を結合\n",
    "    df_small = pd.concat([df_answer_cnt, df_answer_rate, df_segmnet_1_count, df_segmnet_2_count, df_segmnet_3_count], axis=1)\n",
    "    \n",
    "    return df_large, df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 満足度セグメントごとでデータフレームを作成する関数\n",
    "def satisfaction_segment(df):\n",
    "\n",
    "    # 満足度セグメント別回答数\n",
    "    df_segmnet_1 = df[df['segment'] == 'xxx']\n",
    "    df_segmnet_2 = df[df['segment'] == 'yyy']\n",
    "    df_segmnet_3 = df[df['segment'] == 'zzz']\n",
    "    \n",
    "    return df, df_segmnet_1, df_segmnet_2, df_segmnet_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームの特定の列やセルに '\\u0301' が含まれているかどうかを調べる手順\n",
    "# 特定のセルや列に非ASCII文字が含まれているかを確認できる。\n",
    "\n",
    "# 特定の列（例: 'columnName'）に含まれる '\\u0301' を検索\n",
    "offending_rows = df[df['columnName'].str.contains('\\u0301', na=False)]\n",
    "\n",
    "# 全体のデータフレームに対して検索\n",
    "for column in df.columns:\n",
    "    offending_rows = df[df[column].astype(str).str.contains('\\u0301', na=False)]\n",
    "    if not offending_rows.empty:\n",
    "        print(f\"Found '\\u0301' in column '{column}'\")\n",
    "\n",
    "# 全体のデータフレームに対して検索（特定のセルを指定）\n",
    "offending_rows = df[df.applymap(lambda x: '\\u0301' in str(x)).any(axis=1)]\n",
    "\n",
    "# UnicodeEncodeErrorが発生する行を特定し、その行をprint\n",
    "for index, row in df_o.iterrows():\n",
    "    try:\n",
    "        row['xxx'].encode('shift_jis')\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(\"UnicodeEncodeError occurred at index:\", index)\n",
    "        print(\"Row content:\", row['xxx'])\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# 前処理\n",
    "df_copy = nlp_preprocess(df_copy)\n",
    "\n",
    "# 各行のテキスト内で重複する単語を除去する関数の実行\n",
    "df_copy = process_text_column(df_copy)\n",
    "\n",
    "# セグメント別UU\n",
    "df_copy_large, df_copy_small = uu_per_segment(df_copy)\n",
    "\n",
    "# セグメントごとでデータフレームを作成する関数\n",
    "df_copy, df_segment_1, df_segment_2, df_segment_3 = satisfaction_segment(df_copy)\n",
    "\n",
    "# データフレームの保存\n",
    "df_copy_large.to_csv(r\"xxx.csv\", index=False, encoding='shift_jis')\n",
    "df_copy_small.to_csv(r\"yyy.csv\", index=False, encoding='shift_jis')\n",
    "\n",
    "df_copy.to_csv(r\"xxx.csv\", index=False, encoding='shift_jis')\n",
    "df_segment_1.to_csv(r\"xxx.csv\", index=False, encoding='shift_jis')\n",
    "df_segment_2.to_csv(r\"xxx.csv\", index=False, encoding='shift_jis')\n",
    "df_segment_3.to_csv(r\"xxx.csv\", index=False, encoding='shift_jis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献_共起ネットワーク\n",
    "\n",
    "1.Pythonで共起ネットワークを作る方法をご紹介します: https://note.com/noa813/n/ne506f884e467\n",
    "\n",
    "2.(new)【Python】GoogleColab上でNetworkXによる日本語の共起ネットワークを文字化けせずにプロット: https://tkstock.site/2022/08/24/python-googlecolab-networkx-matplotlib-japanese-networkx-plot/\n",
    "\n",
    "3.(old)Pythonを使って文章から共起ネットワークを作る　〜テキストマイニングでの可視化〜: https://www.dskomei.com/entry/2019/04/07/021028\n",
    "\n",
    "4.言語モデル(+カスタムで辞書追加)について: https://zenn.dev/akimen/articles/8d818ca704f079\n",
    "\n",
    "5.spaCyとGiNZAを使った日本語自然言語処理: https://qiita.com/wf-yamaday/items/3ffdcc15a5878b279d61\n",
    "\n",
    "6.transformerとは: https://www.youtube.com/watch?v=QlfhLMUoJjw\n",
    "\n",
    "7.NLP | GINZA v5で固有表現抽出のルール追加を試してみた: https://note.com/lizefield/n/n18fcac42afea\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
